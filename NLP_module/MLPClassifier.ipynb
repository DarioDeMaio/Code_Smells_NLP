{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from  sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset utilizzando pandas\n",
    "dataset = pd.read_csv('../dataset/final_dataset.csv')\n",
    "dataset.drop(['Project_name'], axis=1, inplace=True)\n",
    "# dataset2 = dataset['Component']\n",
    "# y = dataset[['CDSBP','CC','LC','LZC','RB','SC']]\n",
    "\n",
    "df = dataset.to_numpy()\n",
    "X = df[:,0]\n",
    "y = df[:,1:7]\n",
    "y = np.argmax(y, axis=1)\n",
    "\n",
    "# Effettua lo split in train e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   7.1s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.1s\n",
      "Iteration 1, loss = 1.79328281\n",
      "Iteration 2, loss = 1.75729873\n",
      "Iteration 3, loss = 1.72198875\n",
      "Iteration 4, loss = 1.68621643\n",
      "Iteration 5, loss = 1.64596201\n",
      "Iteration 6, loss = 1.59780525\n",
      "Iteration 7, loss = 1.54004399\n",
      "Iteration 8, loss = 1.47540825\n",
      "Iteration 9, loss = 1.40394532\n",
      "Iteration 10, loss = 1.32686500\n",
      "Iteration 11, loss = 1.24583422\n",
      "Iteration 12, loss = 1.15998124\n",
      "Iteration 13, loss = 1.07107938\n",
      "Iteration 14, loss = 0.97485746\n",
      "Iteration 15, loss = 0.87312928\n",
      "Iteration 16, loss = 0.76994589\n",
      "Iteration 17, loss = 0.66971108\n",
      "Iteration 18, loss = 0.58068682\n",
      "Iteration 19, loss = 0.50596078\n",
      "Iteration 20, loss = 0.44744129\n",
      "Iteration 21, loss = 0.40079689\n",
      "Iteration 22, loss = 0.36416324\n",
      "Iteration 23, loss = 0.33355264\n",
      "Iteration 24, loss = 0.30902549\n",
      "Iteration 25, loss = 0.28845749\n",
      "Iteration 26, loss = 0.27304780\n",
      "Iteration 27, loss = 0.26072550\n",
      "Iteration 28, loss = 0.25172902\n",
      "Iteration 29, loss = 0.24320914\n",
      "Iteration 30, loss = 0.23667890\n",
      "Iteration 31, loss = 0.23169802\n",
      "Iteration 32, loss = 0.22738580\n",
      "Iteration 33, loss = 0.22373496\n",
      "Iteration 34, loss = 0.22201864\n",
      "Iteration 35, loss = 0.21850857\n",
      "Iteration 36, loss = 0.21655604\n",
      "Iteration 37, loss = 0.21282188\n",
      "Iteration 38, loss = 0.21137067\n",
      "Iteration 39, loss = 0.20970877\n",
      "Iteration 40, loss = 0.20727445\n",
      "Iteration 41, loss = 0.20888989\n",
      "Iteration 42, loss = 0.20435726\n",
      "Iteration 43, loss = 0.20352446\n",
      "Iteration 44, loss = 0.20165668\n",
      "Iteration 45, loss = 0.20003210\n",
      "Iteration 46, loss = 0.19902322\n",
      "Iteration 47, loss = 0.19640215\n",
      "Iteration 48, loss = 0.19633307\n",
      "Iteration 49, loss = 0.19528529\n",
      "Iteration 50, loss = 0.19399252\n",
      "Iteration 51, loss = 0.19431980\n",
      "Iteration 52, loss = 0.19202962\n",
      "Iteration 53, loss = 0.19021496\n",
      "Iteration 54, loss = 0.19010834\n",
      "Iteration 55, loss = 0.18871078\n",
      "Iteration 56, loss = 0.18873440\n",
      "Iteration 57, loss = 0.18817401\n",
      "Iteration 58, loss = 0.18832369\n",
      "Iteration 59, loss = 0.18644480\n",
      "Iteration 60, loss = 0.18662909\n",
      "Iteration 61, loss = 0.18649472\n",
      "Iteration 62, loss = 0.18444027\n",
      "Iteration 63, loss = 0.18463888\n",
      "Iteration 64, loss = 0.18495215\n",
      "Iteration 65, loss = 0.18295875\n",
      "Iteration 66, loss = 0.18283267\n",
      "Iteration 67, loss = 0.18320456\n",
      "Iteration 68, loss = 0.18268933\n",
      "Iteration 69, loss = 0.18334900\n",
      "Iteration 70, loss = 0.18222766\n",
      "Iteration 71, loss = 0.18173037\n",
      "Iteration 72, loss = 0.18179324\n",
      "Iteration 73, loss = 0.18081602\n",
      "Iteration 74, loss = 0.17907423\n",
      "Iteration 75, loss = 0.18011712\n",
      "Iteration 76, loss = 0.17993760\n",
      "Iteration 77, loss = 0.17859037\n",
      "Iteration 78, loss = 0.18025955\n",
      "Iteration 79, loss = 0.17950447\n",
      "Iteration 80, loss = 0.17900591\n",
      "Iteration 81, loss = 0.17994988\n",
      "Iteration 82, loss = 0.18086362\n",
      "Iteration 83, loss = 0.17900187\n",
      "Iteration 84, loss = 0.17846764\n",
      "Iteration 85, loss = 0.17980991\n",
      "Iteration 86, loss = 0.17906598\n",
      "Iteration 87, loss = 0.17763927\n",
      "Iteration 88, loss = 0.17815024\n",
      "Iteration 89, loss = 0.17762473\n",
      "Iteration 90, loss = 0.17702836\n",
      "Iteration 91, loss = 0.17842372\n",
      "Iteration 92, loss = 0.17651226\n",
      "Iteration 93, loss = 0.17664255\n",
      "Iteration 94, loss = 0.17626668\n",
      "Iteration 95, loss = 0.17723322\n",
      "Iteration 96, loss = 0.17695689\n",
      "Iteration 97, loss = 0.17672473\n",
      "Iteration 98, loss = 0.17656914\n",
      "Iteration 99, loss = 0.17634192\n",
      "Iteration 100, loss = 0.17735698\n",
      "Iteration 101, loss = 0.17912389\n",
      "Iteration 102, loss = 0.18138311\n",
      "Iteration 103, loss = 0.17754001\n",
      "Iteration 104, loss = 0.17602246\n",
      "Iteration 105, loss = 0.17658953\n",
      "Iteration 106, loss = 0.17567088\n",
      "Iteration 107, loss = 0.17565225\n",
      "Iteration 108, loss = 0.17636003\n",
      "Iteration 109, loss = 0.17607726\n",
      "Iteration 110, loss = 0.17612502\n",
      "Iteration 111, loss = 0.17572113\n",
      "Iteration 112, loss = 0.17673559\n",
      "Iteration 113, loss = 0.17565892\n",
      "Iteration 114, loss = 0.17537915\n",
      "Iteration 115, loss = 0.17556595\n",
      "Iteration 116, loss = 0.17632436\n",
      "Iteration 117, loss = 0.17462569\n",
      "Iteration 118, loss = 0.17530103\n",
      "Iteration 119, loss = 0.17517068\n",
      "Iteration 120, loss = 0.17737727\n",
      "Iteration 121, loss = 0.17538644\n",
      "Iteration 122, loss = 0.17546255\n",
      "Iteration 123, loss = 0.17591674\n",
      "Iteration 124, loss = 0.17517191\n",
      "Iteration 125, loss = 0.17534085\n",
      "Iteration 126, loss = 0.17585228\n",
      "Iteration 127, loss = 0.17578762\n",
      "Iteration 128, loss = 0.17397368\n",
      "Iteration 129, loss = 0.17745000\n",
      "Iteration 130, loss = 0.17717313\n",
      "Iteration 131, loss = 0.17558860\n",
      "Iteration 132, loss = 0.17670384\n",
      "Iteration 133, loss = 0.17776707\n",
      "Iteration 134, loss = 0.17627137\n",
      "Iteration 135, loss = 0.17519593\n",
      "Iteration 136, loss = 0.17530748\n",
      "Iteration 137, loss = 0.17651949\n",
      "Iteration 138, loss = 0.17593664\n",
      "Iteration 139, loss = 0.17516310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  17.1s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Vectorize text\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MLPClassifier(verbose=True, hidden_layer_sizes=[10,20,20])),\n",
    "], verbose=True)\n",
    "\n",
    "val = pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score\n",
    "# Evaluate model\n",
    "y_pred = pipe.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8151141098927551"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8507821901323707"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7717601252184046"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
